---
title: "Interacting with Synapse API"
author: "Sage Bionetworks"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    code_fold: show
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(parallel)
library(purrr)
library(reticulate)
library(rlist)
library(stringr)
library(tibble)
library(xml2)
```

# Introduction

1. Pull the data of the 2014 i2b2 NLP De-identification Challenge from Synapse.
2. Convert the dataset to the NLP Sandbox schemas.
3. Push the new dataset to Synapse (training and evaluation).



# Setup

### Requirements

Set the value of `SYNAPSE_TOKEN` in the configuration file `.env` to one of your
Synapse personal access tokens. Token can be generated in Synapse via your User
Menu > Settings > Persona Access Tokens.

### Conda environments

List the Conda environments.

```{r}
options(reticulate.repl.quiet = TRUE)
conda_list(conda = "auto")
```

Activate the environment `i2b2-phi-analysis`.

```{r}
use_condaenv("i2b2-phi-analysis", required = TRUE)
```

### Logging into Synapse

```{r}
synapseclient <- reticulate::import('synapseclient')
syn <- synapseclient$Synapse()
syn$login()
```

### Configuration

```{r}
# NLP Sandbox schemas version
schemas_version <- "1.1.1"
# Output dataset version
dataset_version <- "0.1.0"

# Original files of the 2014 i2b2 NLP De-identification challenge.
dataset_archives = list(
  training_1 = list(
    synId = "syn23196860",  # official-2014_training-PHI-Gold-Set1.tar.gz
    type = "training"
  ),
  training_2 = list(
    synId = "syn23196850",  # official-training-PHI-Gold-Set2.tar.gz
    type = "training"
  ),
  evaluation = list(
    synId = "syn23196857",  # official-testing-PHI-Gold-fixed.tar.gz
    type = "evaluation"
  )
)

dataset_output_syn_folders = list(
  full = list(
    syn_id = "syn25807888"
  ),
  example = list(
    syn_id = "syn25812243"
  )
)

project_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(project_dir, "data")
output_dir <- file.path(project_dir, "output")

# Output
job_name <- "i2b2-phi-dataset-nlpsandbox"
job_version <- dataset_version
job_dir <- file.path(output_dir, job_name)
job_output_dir <- file.path(job_dir, job_version)

# Local outputs
output_files = list(
  training = list(
    # annotations_tsv = file.path(job_output_dir, paste0("training-set-annotations.tsv")),
    annotations_json = file.path(job_output_dir, paste0("training-set-annotations.json")),
    patients_json = file.path(job_output_dir, paste0("training-set-patients.json"))
  ),
  evaluation = list(
    # annotations_tsv = file.path(job_output_dir, paste0("evaluation-set-annotations.tsv")),
    annotations_json = file.path(job_output_dir, paste0("evaluation-set-annotations.json")),
    patients_json = file.path(job_output_dir, paste0("evaluation-set-patients.json"))
  )
)

# Number of CPU cores
num_cores = parallel::detectCores()
```



# Pulling i2b2 dataset from Synapse

Get the dataset archives from Synapse, extract them and get the paths to the XML
files. Each XML file includes the text of the clinical note (TEXT node) and a 
list of annotations (TAGS node).

```{r}
dataset_archives <- lapply(dataset_archives, function(archive) {
  file_handle <- syn$get(archive$synId, downloadLocation=data_dir, 
                         ifcollision='overwrite.local')
  
  # Extract tar.gz archive and get list of XML files
  untar(file_handle$path, exdir=data_dir)
  xml_paths <- file.path(data_dir, untar(file_handle$path, list=TRUE)) %>%
    str_subset(pattern = "\\.xml$")
  
  archive$xml_paths <- xml_paths
  archive
})
```

The training and evaluation datasets are split into multiple parts. Combine 
these parts into training and evaluation datasets. 

```{r}
datasets <- list(
  training = list(
    type = "training",
    xml_paths = list.filter(dataset_archives, type == 'training') %>%
      list.select(xml_paths) %>%
      unlist(use.names = F)
  ),
  evaluation = list(
    type = "evaluation",
    xml_paths = list.filter(dataset_archives, type == 'evaluation') %>%
      list.select(xml_paths) %>%
      unlist(use.names = F)
  )
)
```

Number of clinical notes per dataset:

```{r}
lapply(datasets, function(dataset) { length(dataset$xml_paths) })
```



# Generating i2b2 dataset using NLP Sandbox schemas

Extract the annotations from the XML files and save them to data frames.

```{r}
datasets <- lapply(datasets, function(dataset) {
  annotations <- do.call(rbind, mclapply(dataset$xml_paths, function(xml_path) {
    doc <- xml2::read_xml(xml_path)
    xml_find_all(doc, xpath = "//TAGS/*") %>%
      purrr::map_dfr(~ {
        # save the primary type of the annotations
        name <- xml_name(.)
        
        # attributes of the XML tag
        attrs <- xml_attrs(.) %>% 
          tibble::enframe() %>%
          tidyr::spread(name, value)
        
        # extract patient id and note index from the note filename
        df <- strsplit(fs::path_ext_remove(fs::path_file(xml_path)), "-")[[1]]
        patient_id <- df[1]
        note_index <- df[2]
          
        # combine the annotations
        cbind.data.frame(name, attrs, patient_id, note_index) %>%
          tibble::set_tidy_names() %>% 
          as_tibble()
      })
  }, mc.cores = num_cores, mc.set.seed = TRUE))
  
  dataset$annotations <- annotations
  dataset
})
```

Number of annotations per dataset:

```{r}
lapply(datasets, function(dataset) { nrow(dataset$annotations) })
```


## Preparation

### Setting the type of annotation properties

All the properties are currently strings. The transformation below are applied:

- `start` is set to integer
- `end` is set to integer

```{r}
datasets <- lapply(datasets, function(dataset) {
  dataset$annotations$start <- as.integer(dataset$annotations$start)
  dataset$annotations$end <- as.integer(dataset$annotations$end)
  dataset
})
```

### Replacing annotation property `end` by `length`

The motivation for replacing the property `end` by `length` computed as `end` -
`start` is that the length is more convenient to user and easier to validate
because `length` is relative to `start`.

```{r}
datasets <- lapply(datasets, function(dataset) {
  if ("end" %in% colnames(dataset$annotations)) {
    dataset$annotations$length <- dataset$annotations$end - dataset$annotations$start
    dataset$annotations$end <- NULL
  }
  dataset
})
# datasets$evaluation$annotations
names(datasets$evaluation$annotations)
```


## Generating annotations

### List of annotations

Annotation | Description | NLP Sandbox schema
-------- | ----------- | -------------
`TextDateAnnotation`         | A date annotation in a text.                                | [![HTML notebook](https://img.shields.io/badge/`r schemas_version`-blue.svg?color=1283c3&labelColor=555555&logoColor=ffffff&style=for-the-badge&logo=openapi-initiative)](https://github.com/nlpsandbox/nlpsandbox-schemas/tree/`r schemas_version`/openapi/commons/components/schemas/TextDateAnnotation.yaml)
`TextPersonNameAnnotation`         | A person name annotation in a text.                                | [![HTML notebook](https://img.shields.io/badge/`r schemas_version`-blue.svg?color=1283c3&labelColor=555555&logoColor=ffffff&style=for-the-badge&logo=openapi-initiative)](https://github.com/nlpsandbox/nlpsandbox-schemas/tree/`r schemas_version`/openapi/commons/components/schemas/TextPersonNameAnnotation)
`TextPhysicalAddressAnnotation`         | A physical address annotation in a text.                                | [![HTML notebook](https://img.shields.io/badge/`r schemas_version`-blue.svg?color=1283c3&labelColor=555555&logoColor=ffffff&style=for-the-badge&logo=openapi-initiative)](https://github.com/nlpsandbox/nlpsandbox-schemas/tree/`r schemas_version`/openapi/commons/components/schemas/TextPhysicalAddressAnnotation.yaml)
`TextIdAnnotation`         | An ID annotation in a text.                                | [![HTML notebook](https://img.shields.io/badge/`r schemas_version`-blue.svg?color=1283c3&labelColor=555555&logoColor=ffffff&style=for-the-badge&logo=openapi-initiative)](https://github.com/nlpsandbox/nlpsandbox-schemas/tree/`r schemas_version`/openapi/commons/components/schemas/TextIdAnnotation.yaml)
`TextContactAnnotation`         | A contact annotation in a text.                                | [![HTML notebook](https://img.shields.io/badge/`r schemas_version`-blue.svg?color=1283c3&labelColor=555555&logoColor=ffffff&style=for-the-badge&logo=openapi-initiative)](https://github.com/nlpsandbox/nlpsandbox-schemas/tree/`r schemas_version`/openapi/commons/components/schemas/TextContactAnnotation.yaml)

### Date annotations

```{r}
date_annotations <- lapply(names(datasets), function(dataset_name) {
  dataset <- datasets[[dataset_name]]
  annotations <- dataset$annotations
  annotations <- annotations[annotations$name == "DATE",]
  data.frame(
    noteId = paste0(annotations$patient_id, "-", annotations$note_index),
    annotationType = "Date",
    start = annotations$start,
    length = annotations$length,
    text = annotations$text,
    confidence = 100,
    dateFormat = "",
    stringsAsFactors = FALSE)
})
names(date_annotations) <- names(datasets)
```

### Person name annotations

```{r}
person_name_annotations <- lapply(names(datasets), function(dataset_name) {
  dataset <- datasets[[dataset_name]]
  annotations <- dataset$annotations
  annotations <- annotations[annotations$name == "NAME",]
  data.frame(
    noteId = paste0(annotations$patient_id, "-", annotations$note_index),
    annotationType = "PersonName",
    start = annotations$start,
    length = annotations$length,
    text = annotations$text,
    confidence = 100,
    stringsAsFactors = FALSE)
})
names(person_name_annotations) <- names(datasets)
```


### Physical address annotations

```{r}
# Lookup table used to convert address types
addressTypeLUT <- data.frame(
  i2b2 = c("CITY", 
           "COUNTRY", 
           "HOSPITAL", 
           "LOCATION-OTHER", 
           "ORGANIZATION", 
           "STATE", 
           "STREET", 
           "ZIP"),
  nlp_sandbox = c("city",
                  "country",
                  "hospital",
                  "other",
                  "organization",
                  "state",
                  "street",
                  "zip")
)
physical_address_annotations <- lapply(names(datasets), function(dataset_name) {
  dataset <- datasets[[dataset_name]]
  annotations <- dataset$annotations
  annotations <- annotations[annotations$name == "LOCATION",]
  data.frame(
    noteId = paste0(annotations$patient_id, "-", annotations$note_index),
    annotationType = "PhysicalAddress",
    start = annotations$start,
    length = annotations$length,
    text = annotations$text,
    confidence = 100,
    addressType = addressTypeLUT$nlp_sandbox[match(annotations$TYPE, addressTypeLUT$i2b2)],
    stringsAsFactors = FALSE)
})
names(physical_address_annotations) <- names(datasets)
```

### ID

```{r}
# Lookup table used to convert id types
idTypeLUT <- data.frame(
  i2b2 = c("ACCOUNT", 
           "BIOID", 
           "DEVICE", 
           "HEALTHPLAN", 
           "IDNUM", 
           "LICENSE", 
           "MEDICALRECORD", 
           "SSN",
           "VEHICLE"),
  nlp_sandbox = c("account",
                  "bio_id",
                  "device",
                  "health_plan",
                  "id_number",
                  "license",
                  "medical_record",
                  "ssn",
                  "vehicle")
)
id_annotations <- lapply(names(datasets), function(dataset_name) {
  dataset <- datasets[[dataset_name]]
  annotations <- dataset$annotations
  annotations <- annotations[annotations$name == "ID",]
  data.frame(
    noteId = paste0(annotations$patient_id, "-", annotations$note_index),
    annotationType = "Id",
    start = annotations$start,
    length = annotations$length,
    text = annotations$text,
    confidence = 100,
    idType = idTypeLUT$nlp_sandbox[match(annotations$TYPE, idTypeLUT$i2b2)],
    stringsAsFactors = FALSE)
})
names(id_annotations) <- names(datasets)
```

### Contact information

```{r}
# Lookup table used to convert contact types
contactTypeLUT <- data.frame(
  i2b2 = c("EMAIL", 
           "FAX", 
           "IPADDR", 
           "PHONE", 
           "URL"),
  nlp_sandbox = c("email",
                  "fax",
                  "ip_address",
                  "phone",
                  "url")
)
contact_annotations <- lapply(names(datasets), function(dataset_name) {
  dataset <- datasets[[dataset_name]]
  annotations <- dataset$annotations
  annotations <- annotations[annotations$name == "CONTACT",]
  data.frame(
    noteId = paste0(annotations$patient_id, "-", annotations$note_index),
    annotationType = "Contact",
    start = annotations$start,
    length = annotations$length,
    text = annotations$text,
    confidence = 100,
    contactType = contactTypeLUT$nlp_sandbox[match(annotations$TYPE, contactTypeLUT$i2b2)],
    stringsAsFactors = FALSE)
})
names(contact_annotations) <- names(datasets)
```



# Exporting data to files

### Full dataset

```{r}
# create output dir if needed
dir.create(job_output_dir, recursive = TRUE, showWarnings = FALSE)

# save the patients of the training and evaluation set
noop <- lapply(names(datasets), function(dataset_name) {
  # dataset_name <- "evaluation"
  dataset <- datasets[[dataset_name]]
  patient_ids <- sort(unique(dataset$annotations$patient_id))
  # patient_ids <- patient_ids[1:2]
  patient_bundles <- lapply(patient_ids, function(patient_id) {
    # patient_id <- "110"
    
    # create FHIR Patient object
    patient <- list(
      identifier = patient_id,
      gender = 'unknown'
    )

    # create Notes objects for the patient specified
    tag <- paste0(patient_id, "-")
    xml_paths <- sort(dataset$xml_paths[grepl(tag, dataset$xml_paths)])
    note_bundles <- lapply(xml_paths, function(xml_path){
      # xml_path <- "/home/rstudio/nlp-sandbox-analysis/data/testing-PHI-Gold-fixed/110-01.xml"
      # create note object
      note_xml_doc <- read_xml(xml_path, options = "NOCDATA")
      note_text <- xml_text(xml_find_all(note_xml_doc, "//TEXT"), trim = FALSE)
      note <- list(
        text=note_text,
        noteType="",
        patientId="PATIENT_ID"
      )
      
      # create the Annotation objects for the note specified
      noteId <- gsub(pattern = "\\.xml$", "", basename(xml_path))
    
      ## get date annotations
      df <- date_annotations[[dataset_name]]
      df <- df[df$noteId == noteId,]
      text_date_annotations <- df %>% purrr::transpose()
      
      ## get person name annotations
      df <- person_name_annotations[[dataset_name]]
      df <- df[df$noteId == noteId,]
      text_person_name_annotations <- df %>% purrr::transpose()
      
      ## get physical address annotations
      df <- physical_address_annotations[[dataset_name]]
      df <- df[df$noteId == noteId,]
      text_physical_address_annotations <- df %>% purrr::transpose()
      
      annotation=list(
        annotationSource=list(
          resourceSource=list(
            name="NOTE_NAME"
          )
        ),
        textDateAnnotations=text_date_annotations,
        textPersonNameAnnotations=text_person_name_annotations,
        textPhysicalAddressAnnotations=text_physical_address_annotations
      )

      # create note bundle
      note_bundle <- list(
        note=note,
        annotation=annotation
      )
    })
    
    # create patient bundle
    patient_bundle <- list(
      patient=patient,
      note_bundles=note_bundles
    )
  })
  
  data <- list(
    patient_bundles=patient_bundles
  )
  
  # create output dataset directory and save bundles
  # dataset_dir <- file.path(job_output_dir, dataset_name)
  dir.create(job_output_dir, recursive = TRUE, showWarnings = FALSE)
  data_json <- jsonlite::toJSON(data, auto_unbox = TRUE, pretty = TRUE)
  write(data_json, file.path(job_output_dir, paste0("i2b2-phi-dataset-", dataset_name, ".json")))
})
```

### Preview dataset

```{r}
# create output dir if needed
dir.create(job_output_dir, recursive = TRUE, showWarnings = FALSE)

dataset_name <- "evaluation"
dataset <- datasets[[dataset_name]]
patient_ids <- c("110")

# patient_ids <- patient_ids[1:2]
patient_bundles <- lapply(patient_ids, function(patient_id) {
  # create FHIR Patient object
  patient <- list(
    identifier = patient_id,
    gender = 'unknown'
  )

  # create Notes objects for the patient specified
  tag <- paste0(patient_id, "-")
  xml_paths <- sort(dataset$xml_paths[grepl(tag, dataset$xml_paths)])
  note_bundles <- lapply(xml_paths, function(xml_path){
    # create note object
    note_xml_doc <- read_xml(xml_path, options = "NOCDATA")
    note_text <- xml_text(xml_find_all(note_xml_doc, "//TEXT"), trim = FALSE)
    note <- list(
      text=note_text,
      noteType="",
      patientId="PATIENT_ID"
    )
    
    # create the Annotation objects for the note specified
    noteId <- gsub(pattern = "\\.xml$", "", basename(xml_path))
  
    ## get date annotations
    df <- date_annotations[[dataset_name]]
    df <- df[df$noteId == noteId,]
    text_date_annotations <- df %>% purrr::transpose()
    
    ## get person name annotations
    df <- person_name_annotations[[dataset_name]]
    df <- df[df$noteId == noteId,]
    text_person_name_annotations <- df %>% purrr::transpose()
    
    ## get physical address annotations
    df <- physical_address_annotations[[dataset_name]]
    df <- df[df$noteId == noteId,]
    text_physical_address_annotations <- df %>% purrr::transpose()
    
    annotation=list(
      annotationSource=list(
        resourceSource=list(
          name="NOTE_NAME"
        )
      ),
      textDateAnnotations=text_date_annotations,
      textPersonNameAnnotations=text_person_name_annotations,
      textPhysicalAddressAnnotations=text_physical_address_annotations
    )

    # create note bundle
    note_bundle <- list(
      note=note,
      annotation=annotation
    )
  })
  
  # create patient bundle
  patient_bundle <- list(
    patient=patient,
    note_bundles=note_bundles
  )
})

data <- list(
  patient_bundles=patient_bundles
)

# create output dataset directory and save bundles
# dataset_dir <- file.path(job_output_dir, dataset_name)
dir.create(job_output_dir, recursive = TRUE, showWarnings = FALSE)
data_json <- jsonlite::toJSON(data, auto_unbox = TRUE, pretty = TRUE)
write(data_json, file.path(job_output_dir, paste0("i2b2-phi-dataset-example.json")))
```



# Exporting data files to Synapse

### Full dataset

```{python}
from os import walk, path
import synapseclient
from synapseclient import Project, Folder, File, Link

print(r.output_dir)
syn = synapseclient.Synapse()
syn.login()

# This is the output directory on Synapse
output_dir = syn.get(output_dir_syn_id)

# Create the job directory
job_output_dir = Folder(r.job_version, parent=output_dir)
job_output_dir = syn.store(job_output_dir)
for dirpath, dirnames, filenames in walk(r.job_output_dir):
  for filename in filenames:
    filepath = path.join(dirpath, filename)
    print('Pushing to Synapse %s' % path.join(dirpath, filename))
    file = File(filepath, parent=job_output_dir)
    file = syn.store(file)
```
